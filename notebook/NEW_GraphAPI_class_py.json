{
	"name": "NEW_GraphAPI_class_py",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "55a483dd-c10e-4be0-8f70-d062f3ffa730"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"class GraphAPI(BaseOEAModule):\r\n",
					"    def __init__(self, oea, source_folder='GraphAPI', pseudonymize = True):\r\n",
					"        BaseOEAModule.__init__(self, oea, source_folder, pseudonymize)\r\n",
					"\r\n",
					"        self.stage1np_graphapi_test_users = self.stage1np + '/GraphAPI/test_data/Users'\r\n",
					"        self.stage1np_graphapi_test_m365 = self.stage1np + '/GraphAPI/test_data/M365_App_User_Detail'\r\n",
					"        self.stage1np_graphapi_test_teams = self.stage1np + '/GraphAPI/test_data/Teams_Activity_User_Detail'\r\n",
					"\r\n",
					"        self.stage1np_graphapi_users = self.stage1np + '/GraphAPI/Users'\r\n",
					"        self.stage1np_graphapi_m365 = self.stage1np + '/GraphAPI/M365_App_User_Detail'\r\n",
					"        self.stage1np_graphapi_teams = self.stage1np + '/GraphAPI/Teams_Activity_User_Detail'\r\n",
					"\r\n",
					"        self.schemas['users'] = [['surname', 'string', 'mask'],\r\n",
					"                                ['givenName', 'string', 'mask'],\r\n",
					"                                ['userPrincipalName', 'string', 'hash'],\r\n",
					"                                ['id', 'string', 'mask']]\r\n",
					"\r\n",
					"        self.schemas['m365'] = [['reportRefreshDate', 'date', 'no-op'],\r\n",
					"                                ['userPrincipalName', 'string', 'hash'],\r\n",
					"                                ['lastActivityDate', 'date', 'no-op'],\r\n",
					"                                ['reportPeriod', 'string', 'no-op'],\r\n",
					"                                ['mobile', 'boolean', 'no-op'],\r\n",
					"                                ['web', 'boolean', 'no-op'],\r\n",
					"                                ['mac', 'boolean', 'no-op'],\r\n",
					"                                ['windows', 'boolean', 'no-op'],\r\n",
					"                                ['excel', 'boolean', 'no-op'],\r\n",
					"                                ['excelMac', 'boolean', 'no-op'],\r\n",
					"                                ['excelMobile', 'boolean', 'no-op'],\r\n",
					"                                ['excelWeb', 'boolean', 'no-op'],\r\n",
					"                                ['excelWindows', 'boolean', 'no-op'],\r\n",
					"                                ['oneNote', 'boolean', 'no-op'],\r\n",
					"                                ['oneNoteMac', 'boolean', 'no-op'],\r\n",
					"                                ['oneNoteMobile', 'boolean', 'no-op'],\r\n",
					"                                ['oneNoteWeb', 'boolean', 'no-op'],\r\n",
					"                                ['oneNoteWindows', 'boolean', 'no-op'],\r\n",
					"                                ['outlook', 'boolean', 'no-op'],\r\n",
					"                                ['outlookMac', 'boolean', 'no-op'],\r\n",
					"                                ['outlookMobile', 'boolean', 'no-op'],\r\n",
					"                                ['outlookWeb', 'boolean', 'no-op'],\r\n",
					"                                ['outlookWindows', 'boolean', 'no-op'],\r\n",
					"                                ['powerPoint', 'boolean', 'no-op'],\r\n",
					"                                ['powerPointMac', 'boolean', 'no-op'],\r\n",
					"                                ['powerPointMobile', 'boolean', 'no-op'],\r\n",
					"                                ['powerPointWeb', 'boolean', 'no-op'],\r\n",
					"                                ['powerPointWindows', 'boolean', 'no-op'],\r\n",
					"                                ['teams', 'boolean', 'no-op'],\r\n",
					"                                ['teamsMac', 'boolean', 'no-op'],\r\n",
					"                                ['teamsMobile', 'boolean', 'no-op'],\r\n",
					"                                ['teamsWeb', 'boolean', 'no-op'],\r\n",
					"                                ['teamsWindows', 'boolean', 'no-op'],\r\n",
					"                                ['word', 'boolean', 'no-op'],\r\n",
					"                                ['wordMac', 'boolean', 'no-op'],\r\n",
					"                                ['wordMobile', 'boolean', 'no-op'],\r\n",
					"                                ['wordWeb', 'boolean', 'no-op'],\r\n",
					"                                ['wordWindows', 'boolean', 'no-op']]\r\n",
					"\r\n",
					"        self.schemas['teams'] = [['reportRefreshDate', 'string', 'no-op'],\r\n",
					"                                ['lasActivityDate', 'string', 'no-op'],\r\n",
					"                                ['deletedDate', 'integer', 'no-op'],\r\n",
					"                                ['isDeleted', 'string', 'no-op'],\r\n",
					"                                ['isLiscenced', 'string', 'no-op'],                        \r\n",
					"                                ['reportPeriod', 'string', 'no-op'],\r\n",
					"                                ['userPrincipalName', 'string', 'hash'],\r\n",
					"                                ['privateChatMessageCount', 'integer', 'no-op'],\r\n",
					"                                ['teamChatMessageCount', 'integer', 'no-op'],\r\n",
					"                                ['meetingsAttendedCount', 'integer', 'no-op'],\r\n",
					"                                ['meetingCount', 'integer', 'no-op'],\r\n",
					"                                ['meetingsOrganizedCount', 'integer', 'no-op'],                        \r\n",
					"                                ['callCount', 'integer', 'no-op'],\r\n",
					"                                ['audioDuration', 'string', 'no-op'],\r\n",
					"                                ['videoDuration', 'string', 'no-op'],\r\n",
					"                                ['screenShareDuration', 'string', 'no-op'],                        \r\n",
					"                                ['scheduledOneTimeMeetingsAttendedCount', 'integer', 'no-op'],\r\n",
					"                                ['scheduledOneTimeMeetingsOrganizedCount', 'string', 'no-op'],\r\n",
					"                                ['scheduledRecurringMeetingsAttendedCount', 'string', 'no-op'],\r\n",
					"                                ['scheduledRecurringMeetingsOrganizedCount', 'string', 'no-op'],\r\n",
					"                                ['adHocMeetingsAttendedCount', 'string', 'no-op'],\r\n",
					"                                ['adHocMeetingsOrganizedCount', 'string', 'no-op'],\r\n",
					"                                ['assignedProducts', 'string', 'no-op'],\r\n",
					"                                ['hasOtherAction', 'string', 'no-op']]\r\n",
					"    \r\n",
					"    def process_test_data_from_stage1(self):\r\n",
					"        \"\"\" Processes graphapi test data from stage1 into stage2 \"\"\"\r\n",
					"        \r\n",
					"        # process users test data\r\n",
					"        logger.info(\"Processing microsoft_graph_users test_data from: \" + self.stage1np_graphapi_test_users)\r\n",
					"        users_spark_schema = self.oea.to_spark_schema(self.schemas['users'])\r\n",
					"        dfUsers = spark.read.format('json').load(self.stage1np_graphapi_test_users + '/*.json', schema=users_spark_schema)\r\n",
					"        dfUsers = dfUsers.select(explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        #dfUsers.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/GraphAPI/test_data/Users')\r\n",
					"        df_pseudo, df_lookup = self.oea.pseudonymize(dfUsers, users_spark_schema)\r\n",
					"        df_pseudo.write.format('parquet').mode('overwrite').save(self.stage2p + '/GraphAPI/test_data/Users')\r\n",
					"        df_lookup.write.format('parquet').mode('overwrite').save(self.stage2np + '/GraphAPI/test_data/Users')\r\n",
					"        \r\n",
					"        # process m365 test data\r\n",
					"        logger.info(\"Processing microsoft_graph_m365 test_data from: \" + self.stage1np_graphapi_test_m365)\r\n",
					"        m365_spark_schema = self.oea.to_spark_schema(self.schemas['m365'])\r\n",
					"        dfM365 = spark.read.format('json').load(self.stage1np_graphapi_test_m365 + '/*.json', schema=m365_spark_schema)\r\n",
					"            # explode the details from the m365 json\r\n",
					"        dfM365 = dfM365.select(explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        dfM365 = dfM365.withColumn('reportPeriod', F.explode(F.col('details').reportPeriod)) \\\r\n",
					"                        .withColumn('mobile', F.explode(F.col('details').mobile)) \\\r\n",
					"                        .withColumn('web', F.explode(F.col('details').web)) \\\r\n",
					"                        .withColumn('mac', F.explode(F.col('details').mac)) \\\r\n",
					"                        .withColumn('windows', F.explode(F.col('details').windows)) \\\r\n",
					"                        .withColumn('excel', F.explode(F.col('details').excel)) \\\r\n",
					"                        .withColumn('excelMobile', F.explode(F.col('details').excelMobile)) \\\r\n",
					"                        .withColumn('excelWeb', F.explode(F.col('details').excelWeb)) \\\r\n",
					"                        .withColumn('excelMac', F.explode(F.col('details').excelMac)) \\\r\n",
					"                        .withColumn('excelWindows', F.explode(F.col('details').excelWindows)) \\\r\n",
					"                        .withColumn('oneNote', F.explode(F.col('details').oneNote)) \\\r\n",
					"                        .withColumn('oneNoteMobile', F.explode(F.col('details').oneNoteMobile)) \\\r\n",
					"                        .withColumn('oneNoteWeb', F.explode(F.col('details').oneNoteWeb)) \\\r\n",
					"                        .withColumn('oneNoteMac', F.explode(F.col('details').oneNoteMac)) \\\r\n",
					"                        .withColumn('oneNoteWindows', F.explode(F.col('details').oneNoteWindows)) \\\r\n",
					"                        .withColumn('outlook', F.explode(F.col('details').outlook)) \\\r\n",
					"                        .withColumn('outlookMobile', F.explode(F.col('details').outlookMobile)) \\\r\n",
					"                        .withColumn('outlookWeb', F.explode(F.col('details').outlookWeb)) \\\r\n",
					"                        .withColumn('outlookMac', F.explode(F.col('details').outlookMac)) \\\r\n",
					"                        .withColumn('outlookWindows', F.explode(F.col('details').outlookWindows)) \\\r\n",
					"                        .withColumn('powerPoint', F.explode(F.col('details').powerPoint)) \\\r\n",
					"                        .withColumn('powerPointMobile', F.explode(F.col('details').powerPointMobile)) \\\r\n",
					"                        .withColumn('powerPointWeb', F.explode(F.col('details').powerPointWeb)) \\\r\n",
					"                        .withColumn('powerPointMac', F.explode(F.col('details').powerPointMac)) \\\r\n",
					"                        .withColumn('powerPointWindows', F.explode(F.col('details').powerPointWindows)) \\\r\n",
					"                        .withColumn('teams', F.explode(F.col('details').teams)) \\\r\n",
					"                        .withColumn('teamsMobile', F.explode(F.col('details').teamsMobile)) \\\r\n",
					"                        .withColumn('teamsWeb', F.explode(F.col('details').teamsWeb)) \\\r\n",
					"                        .withColumn('teamsMac', F.explode(F.col('details').teamsMac)) \\\r\n",
					"                        .withColumn('teamsWindows', F.explode(F.col('details').teamsWindows)) \\\r\n",
					"                        .withColumn('word', F.explode(F.col('details').word)) \\\r\n",
					"                        .withColumn('wordMobile', F.explode(F.col('details').wordMobile)) \\\r\n",
					"                        .withColumn('wordWeb', F.explode(F.col('details').wordWeb)) \\\r\n",
					"                        .withColumn('wordMac', F.explode(F.col('details').wordMac)) \\\r\n",
					"                        .withColumn('wordWindows', F.explode(F.col('details').wordWindows)) \\\r\n",
					"                        .drop('details')\r\n",
					"            # change columns with dates to be of date types\r\n",
					"        dfM365 = dfM365.withColumn('reportRefreshDate', to_date(col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
					"        dfM365 = dfM365.withColumn('lastActivityDate', to_date(col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
					"            # write back to the lake in stage 2 ds2_main directory\r\n",
					"        dfM365 = dfM365.withColumn('lastActivationDate', to_date(col('lastActivationDate'), 'yyyy-MM-dd'))\r\n",
					"        #dfM365.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/GraphAPI/test_data/M365_App_User_Detail')\r\n",
					"        df_pseudo, df_lookup = self.oea.pseudonymize(dfM365, m365_spark_schema)\r\n",
					"        df_pseudo.write.format('parquet').mode('overwrite').save(self.stage2p + '/GraphAPI/test_data/M365_App_User_Detail')\r\n",
					"        df_lookup.write.format('parquet').mode('overwrite').save(self.stage2np + '/GraphAPI/test_data/M365_App_User_Detail')\r\n",
					"\r\n",
					"        # process teams test data\r\n",
					"        logger.info(\"Processing microsoft_graph_teams test_data from: \" + self.stage1np_graphapi_teams)\r\n",
					"        teams_spark_schema = self.oea.to_spark_schema(self.schemas['teams'])\r\n",
					"        dfTeams = spark.read.format('json').load(self.stage1np_graphapi_teams + '/*.json', schema=teams_spark_schema)\r\n",
					"        dfTeams = dfTeams.select(explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        dfTeams = dfTeams.withColumn('assignedProducts', F.explode(F.col('assignedProducts')))\r\n",
					"        dfTeams = dfTeams.drop('@odata.type')\r\n",
					"        dfTeams = dfTeams.select('userPrincipalName','lastActivityDate','reportRefreshDate', 'reportPeriod','isDeleted', 'isLicensed', \r\n",
					"                                'deletedDate', 'hasOtherAction', 'assignedProducts',\r\n",
					"                                'adHocMeetingsAttendedCount', 'adHocMeetingsOrganizedCount',  'callCount', 'meetingCount',\r\n",
					"                                'meetingsAttendedCount', 'meetingsOrganizedCount', 'privateChatMessageCount',  'scheduledOneTimeMeetingsAttendedCount', \r\n",
					"                                'scheduledOneTimeMeetingsOrganizedCount', 'scheduledRecurringMeetingsAttendedCount', 'scheduledRecurringMeetingsOrganizedCount',\r\n",
					"                                'screenShareDuration', 'teamChatMessageCount', 'videoDuration','audioDuration')\r\n",
					"            # convert duration to seconds only\r\n",
					"        dfTeams = dfTeams.withColumn(\r\n",
					"            'screenShareDuration', \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            ).withColumn(\r\n",
					"            'videoDuration', \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            ).withColumn(\r\n",
					"            'audioDuration', \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            )\r\n",
					"            # write back to the lake in stage 2 ds2_main directory\r\n",
					"        dfTeams = dfTeams.withColumn('reportRefreshDate', to_date(col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
					"        dfTeams = dfTeams.withColumn('deletedDate', to_date(col('deletedDate'), 'yyyy-MM-dd'))\r\n",
					"        #dfTeams.write.format('parquet').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/GraphAPI/test_data/Teams_Activity_User_Detail')\r\n",
					"        df_pseudo, df_lookup = self.oea.pseudonymize(dfTeams, teams_spark_schema)\r\n",
					"        df_pseudo.write.format('parquet').mode('overwrite').save(self.stage2p + '/GraphAPI/test_data/Teams_Activity_User_Detail')\r\n",
					"        df_lookup.write.format('parquet').mode('overwrite').save(self.stage2np + '/GraphAPI/test_data/Teams_Activity_User_Detail')\r\n",
					"        \r\n",
					"        logger.info(\"Finished processing test data from stage 1 to stage 2\")\r\n",
					"\r\n",
					"    # Unsure whether to use structured streaming or the other method Gene used for the ms_insights module\r\n",
					"\r\n",
					"    def process_data_from_stage1(self):\r\n",
					"        \"\"\" Processes all graph api data in stage1 and writes out to stage2 and stage2p \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph data from: \" + self.stage1np)\r\n",
					"\r\n",
					"        items = mssparkutils.fs.ls(self.stage1np + '/GraphAPI')\r\n",
					"        for item in items:\r\n",
					"            if item.isDir:\r\n",
					"                self._process_graphapi_date_folder(item.path)\r\n",
					"                mssparkutils.fs.mv(item.path, self.stage1np + '/graphapi_processed/', True)\r\n",
					"    \r\n",
					"    def _process_graphapi_date_folder(self, date_folder_path):\r\n",
					"        folders = self.oea.get_folders(date_folder_path)\r\n",
					"        for table_name in folders:\r\n",
					"            self._process_roster_entity(date_folder_path, table_name)\r\n",
					"\r\n",
					"    def _process_graphapi_entity(self, path, entity):\r\n",
					"        try:\r\n",
					"            logger.debug(f\"Processing roster entity: path={path}, entity={entity}\")\r\n",
					"            spark_schema = self.oea.to_spark_schema(self.schemas[entity])\r\n",
					"            df = spark.read.format('json').load(path + '/' + entity, header='true', schema=spark_schema)\r\n",
					"            df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas[entity])\r\n",
					"\r\n",
					"            if len(df_pseudo.columns) > 0: \r\n",
					"                df_pseudo.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2p + '/' + entity)\r\n",
					"            if len(df_lookup.columns) > 0: \r\n",
					"                df_lookup.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(self.stage2np + '/' + entity + '_lookup')\r\n",
					"\r\n",
					"        except (AnalysisException) as error:\r\n",
					"            logger.exception(str(error))\r\n",
					"    \r\n",
					"    # This is the alternate route \r\n",
					"    def process_graphapi_data_from_stage1(self):\r\n",
					"        \"\"\" Processes graphapi data from stage1 into stage2 using structured streaming within the defined functions below. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph data from: \" + self.stage1np)\r\n",
					"\r\n",
					"        items = mssparkutils.fs.ls(self.stage1np + '/GraphAPI')\r\n",
					"        for item in items:\r\n",
					"            self._process_graphapi_users_stage1_data()\r\n",
					"            self._process_graphapi_m365_stage1_data()\r\n",
					"            self._process_graphapi_teams_stage1_data()\r\n",
					"        \r\n",
					"        logger.info(\"Finished processing graphapi data from stage 1 to stage 2\")\r\n",
					"\r\n",
					"    def _process_graphapi_users_stage1_data(self):\r\n",
					"        \"\"\" Processes users data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph users data from: \" + self.stage1np_graphapi_users)\r\n",
					"\r\n",
					"        spark_schema = self.oea.to_spark_schema(self.schemas['users'])\r\n",
					"        df = spark.readStream.format('json').load(self.stage1np_graphapi_users + '/*/*.json', header='true', schema=spark_schema)\r\n",
					"\r\n",
					"        df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas['users'])\r\n",
					"\r\n",
					"        query = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_users + '/_non-pseudo_checkpoints')\r\n",
					"        query = query.start(self.stage2np + '/GraphAPI/Users')\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_users + '/_pseudo_checkpoints')\r\n",
					"        query = query.start(self.stage2p + '/GraphAPI/Users')\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        logger.info(query.lastProgress)\r\n",
					"\r\n",
					"    def _process_graphapi_m365_stage1_data(self):\r\n",
					"        \"\"\" Processes m365 data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph m365 data from: \" + self.stage1np_graphapi_m365)\r\n",
					"\r\n",
					"        spark_schema = self.oea.to_spark_schema(self.schemas['m365'])\r\n",
					"        df = spark.readStream.format('json').load(self.stage1np_graphapi_m365 + '/*/*.json', header='true', schema=spark_schema)\r\n",
					"\r\n",
					"        # explode the details from the m365 json\r\n",
					"        df = df.select(explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        df = df.withColumn('reportPeriod', F.explode(F.col('details').reportPeriod)) \\\r\n",
					"                        .withColumn('mobile', F.explode(F.col('details').mobile)) \\\r\n",
					"                        .withColumn('web', F.explode(F.col('details').web)) \\\r\n",
					"                        .withColumn('mac', F.explode(F.col('details').mac)) \\\r\n",
					"                        .withColumn('windows', F.explode(F.col('details').windows)) \\\r\n",
					"                        .withColumn('excel', F.explode(F.col('details').excel)) \\\r\n",
					"                        .withColumn('excelMobile', F.explode(F.col('details').excelMobile)) \\\r\n",
					"                        .withColumn('excelWeb', F.explode(F.col('details').excelWeb)) \\\r\n",
					"                        .withColumn('excelMac', F.explode(F.col('details').excelMac)) \\\r\n",
					"                        .withColumn('excelWindows', F.explode(F.col('details').excelWindows)) \\\r\n",
					"                        .withColumn('oneNote', F.explode(F.col('details').oneNote)) \\\r\n",
					"                        .withColumn('oneNoteMobile', F.explode(F.col('details').oneNoteMobile)) \\\r\n",
					"                        .withColumn('oneNoteWeb', F.explode(F.col('details').oneNoteWeb)) \\\r\n",
					"                        .withColumn('oneNoteMac', F.explode(F.col('details').oneNoteMac)) \\\r\n",
					"                        .withColumn('oneNoteWindows', F.explode(F.col('details').oneNoteWindows)) \\\r\n",
					"                        .withColumn('outlook', F.explode(F.col('details').outlook)) \\\r\n",
					"                        .withColumn('outlookMobile', F.explode(F.col('details').outlookMobile)) \\\r\n",
					"                        .withColumn('outlookWeb', F.explode(F.col('details').outlookWeb)) \\\r\n",
					"                        .withColumn('outlookMac', F.explode(F.col('details').outlookMac)) \\\r\n",
					"                        .withColumn('outlookWindows', F.explode(F.col('details').outlookWindows)) \\\r\n",
					"                        .withColumn('powerPoint', F.explode(F.col('details').powerPoint)) \\\r\n",
					"                        .withColumn('powerPointMobile', F.explode(F.col('details').powerPointMobile)) \\\r\n",
					"                        .withColumn('powerPointWeb', F.explode(F.col('details').powerPointWeb)) \\\r\n",
					"                        .withColumn('powerPointMac', F.explode(F.col('details').powerPointMac)) \\\r\n",
					"                        .withColumn('powerPointWindows', F.explode(F.col('details').powerPointWindows)) \\\r\n",
					"                        .withColumn('teams', F.explode(F.col('details').teams)) \\\r\n",
					"                        .withColumn('teamsMobile', F.explode(F.col('details').teamsMobile)) \\\r\n",
					"                        .withColumn('teamsWeb', F.explode(F.col('details').teamsWeb)) \\\r\n",
					"                        .withColumn('teamsMac', F.explode(F.col('details').teamsMac)) \\\r\n",
					"                        .withColumn('teamsWindows', F.explode(F.col('details').teamsWindows)) \\\r\n",
					"                        .withColumn('word', F.explode(F.col('details').word)) \\\r\n",
					"                        .withColumn('wordMobile', F.explode(F.col('details').wordMobile)) \\\r\n",
					"                        .withColumn('wordWeb', F.explode(F.col('details').wordWeb)) \\\r\n",
					"                        .withColumn('wordMac', F.explode(F.col('details').wordMac)) \\\r\n",
					"                        .withColumn('wordWindows', F.explode(F.col('details').wordWindows)) \\\r\n",
					"                        .drop('details')\r\n",
					"        # change columns with dates to be of date types\r\n",
					"        df = df.withColumn('reportRefreshDate', to_date(col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
					"        df = df.withColumn('lastActivityDate', to_date(col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
					"        df = df.withColumn('lastActivationDate', to_date(col('lastActivationDate'), 'yyyy-MM-dd'))\r\n",
					"\r\n",
					"        df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas['m365'])\r\n",
					"\r\n",
					"        query = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_m365 + '/_non-pseudo_checkpoints')\r\n",
					"        query = query.start(self.stage2np + '/GraphAPI/M365_App_User_Detail')\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_m365 + '/_pseudo_checkpoints')\r\n",
					"        query = query.start(self.stage2p + '/GraphAPI/M365_App_User_Detail')\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        logger.info(query.lastProgress)\r\n",
					"\r\n",
					"    def _process_graphapi_teams_stage1_data(self):\r\n",
					"        \"\"\" Processes teams data from stage1 into stage2 using structured streaming. \"\"\"\r\n",
					"        logger.info(\"Processing microsoft_graph teams data from: \" + self.stage1np_graphapi_teams)\r\n",
					"\r\n",
					"        spark_schema = self.oea.to_spark_schema(self.schemas['teams'])\r\n",
					"        df = spark.readStream.format('json').load(self.stage1np_graphapi_teams + '/*/*.json', header='true', schema=spark_schema)\r\n",
					"        \r\n",
					"        # explode teams details\r\n",
					"        df = df.select(explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
					"        df = df.withColumn('assignedProducts', F.explode(F.col('assignedProducts')))\r\n",
					"        df = dfTeams.drop('@odata.type')\r\n",
					"        df = df.select('userPrincipalName','lastActivityDate','reportRefreshDate', 'reportPeriod','isDeleted', 'isLicensed', \r\n",
					"                        'deletedDate', 'hasOtherAction', 'assignedProducts',\r\n",
					"                        'adHocMeetingsAttendedCount', 'adHocMeetingsOrganizedCount',  'callCount', 'meetingCount',\r\n",
					"                        'meetingsAttendedCount', 'meetingsOrganizedCount', 'privateChatMessageCount',  'scheduledOneTimeMeetingsAttendedCount', \r\n",
					"                        'scheduledOneTimeMeetingsOrganizedCount', 'scheduledRecurringMeetingsAttendedCount', 'scheduledRecurringMeetingsOrganizedCount',\r\n",
					"                        'screenShareDuration', 'teamChatMessageCount', 'videoDuration','audioDuration')\r\n",
					"        # convert duration to seconds only\r\n",
					"        df = df.withColumn(\r\n",
					"            'screenShareDuration', \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            ).withColumn(\r\n",
					"            'videoDuration', \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            ).withColumn(\r\n",
					"            'audioDuration', \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
					"            F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
					"            )\r\n",
					"        # change columns with dates to be of date types\r\n",
					"        df = df.withColumn('reportRefreshDate', to_date(col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
					"        df = df.withColumn('deletedDate', to_date(col('deletedDate'), 'yyyy-MM-dd'))\r\n",
					"\r\n",
					"        df_pseudo, df_lookup = self.oea.pseudonymize(df, self.schemas['teams'])\r\n",
					"\r\n",
					"        query = df_lookup.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_teams + '/_non-pseudo_checkpoints')\r\n",
					"        query = query.start(self.stage2np + '/GraphAPI/Teams_Activity_User_Detail')\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        query = df_pseudo.writeStream.format(\"delta\").outputMode(\"append\").trigger(once=True).option(\"checkpointLocation\", self.stage1np_graphapi_teams + '/_pseudo_checkpoints')\r\n",
					"        query = query.start(self.stage2p + '/GraphAPI/Teams_Activity_User_Detail')\r\n",
					"        query.awaitTermination()   # block until query is terminated, with stop() or with error; A StreamingQueryException will be thrown if an exception occurs.\r\n",
					"        logger.info(query.lastProgress)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"attachments": null,
				"execution_count": null
			}
		]
	}
}